{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c354381c-d046-464c-9fc6-295386ef2f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost pandas mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7ba22d-bc2b-4c6e-a7c1-89f121cf92df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ac072d-3d15-4959-872f-ade854593cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"workspace.churn_zero.churn_risk_model_xgboost\" \n",
    "loaded_model = mlflow.xgboost.load_model(f\"models:/{model_name}@n3\") \n",
    "\n",
    "print(\"Modelo carregado do Model Registry/Unity Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6eaeac-d10c-42bf-90dd-52b33a77605a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Workspace/Users/joaovab@al.insper.edu.br/Hackathon_Terra_Signal_grupo_1/model/feature_names.json\", \"r\") as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "print(\"Features carregadas com sucesso!\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51183955-b656-4ad6-94aa-33a78b0e8f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_inferencia_spark = spark.table(\"workspace.churn_zero.inference_gold\")\n",
    "df_inferencia_pandas = df_inferencia_spark.toPandas()\n",
    "\n",
    "customer_ids = df_inferencia_pandas['customerID']\n",
    "\n",
    "for c in df_inferencia_pandas.columns:\n",
    "  if c not in feature_names:\n",
    "    df_inferencia_pandas = df_inferencia_pandas.drop(columns=[c])\n",
    "\n",
    "for c in feature_names:\n",
    "  if c not in df_inferencia_pandas.columns:\n",
    "    df_inferencia_pandas[c] = 0\n",
    "\n",
    "\n",
    "\n",
    "selected_features = [\n",
    "    col for col in feature_names\n",
    "    if col not in ['customerID', 'MonthlyCharges']\n",
    "]\n",
    "X_inferencia = df_inferencia_pandas[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837ead41-30c6-42eb-be73-7c0559554ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_probabilities = loaded_model.predict_proba(X_inferencia)[:, 1]\n",
    "\n",
    "results_df_pandas = pd.DataFrame({\n",
    "    'customerID': customer_ids, \n",
    "    'churn_probability': churn_probabilities,\n",
    "    'churn': np.round(churn_probabilities, 0),\n",
    "})\n",
    "\n",
    "results_df_pandas['prediction'] = results_df_pandas['churn'].apply(lambda x: 'Yes' if x == 1 else 'No')\n",
    "\n",
    "results_df_spark = spark.createDataFrame(results_df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f4415f-54d5-4062-b817-a0bcd7e05260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tabela_original = spark.table(\"workspace.churn_zero.inference_gold\") \n",
    "\n",
    "df_com_probabilidade = tabela_original.join(\n",
    "    results_df_spark,\n",
    "    on=\"customerID\",\n",
    "    how=\"inner\" \n",
    ")\n",
    "\n",
    "print(\"\\nPr√©via da Tabela Original com a Coluna de Probabilidade Adicionada:\")\n",
    "df_com_probabilidade.select(\"customerID\", \"Partner\", \"Churn_Probability\").show(5)\n",
    "\n",
    "\n",
    "CATALOGO_FINAL = \"workspace\"\n",
    "SCHEMA_FINAL = \"churn_zero\"\n",
    "TABELA_INFERENCIA_FINAL = \"inference_churn\"\n",
    "\n",
    "df_com_probabilidade.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{CATALOGO_FINAL}.{SCHEMA_FINAL}.{TABELA_INFERENCIA_FINAL}\")\n",
    "print(f\"\\nTabela final salva em: {TABELA_INFERENCIA_FINAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96e193a-b659-4c6f-a1df-329e5b45494c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_inference_churn = spark.table(\"workspace.churn_zero.inference_churn\")\n",
    "df_inference_churn.toPandas().to_csv(\"../prediction.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_predict",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
