{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c354381c-d046-464c-9fc6-295386ef2f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install xgboost pandas mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c7ba22d-bc2b-4c6e-a7c1-89f121cf92df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ac072d-3d15-4959-872f-ade854593cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se você registrou o modelo como 'seu_catalog.seu_schema.churn_risk_model'\n",
    "model_name = \"workspace.churn_zero.churn_risk_model_xgboost\" \n",
    "loaded_model = mlflow.xgboost.load_model(f\"models:/{model_name}@n2\") \n",
    "\n",
    "print(\"Modelo carregado do Model Registry/Unity Catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6eaeac-d10c-42bf-90dd-52b33a77605a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/Workspace/Users/joaovab@al.insper.edu.br/Hackathon_Terra_Signal_grupo_1/feature_names.json\", \"r\") as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "print(\"Features carregadas com sucesso!\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51183955-b656-4ad6-94aa-33a78b0e8f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_inferencia_spark = spark.table(\"workspace.churn_zero.inference_gold\")\n",
    "df_inferencia_pandas = df_inferencia_spark.toPandas()\n",
    "\n",
    "df_inferencia_pandas['feedback_topic_service_quality'] = 0\n",
    "\n",
    "customer_ids = df_inferencia_pandas['customerID']\n",
    "\n",
    "# Remove 'customerID' and 'MonthlyCharges' from feature_names before selection\n",
    "selected_features = [\n",
    "    col for col in feature_names\n",
    "    if col not in ['customerID', 'MonthlyCharges']\n",
    "]\n",
    "X_inferencia = df_inferencia_pandas[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2405eb90-1f5f-4d6e-9d5a-74c98f033691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for c in df_inferencia_pandas.columns:\n",
    "    if c not in feature_names:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837ead41-30c6-42eb-be73-7c0559554ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "churn_probabilities = loaded_model.predict_proba(X_inferencia)[:, 1]\n",
    "\n",
    "# Crie o DataFrame Pandas com as duas colunas-chave\n",
    "results_df_pandas = pd.DataFrame({\n",
    "    'customerID': df_inferencia_pandas['customerID'], # Use o ID do cliente como chave\n",
    "    'churn_probability': churn_probabilities\n",
    "})\n",
    "\n",
    "# --- 2. Converter o DF Pandas para Spark ---\n",
    "# Esta conversão é necessária para realizar o JOIN com a tabela Spark original.\n",
    "results_df_spark = spark.createDataFrame(results_df_pandas)\n",
    "\n",
    "print(\"DataFrame de resultados em formato Spark criado.\")\n",
    "results_df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f4415f-54d5-4062-b817-a0bcd7e05260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Carregar a Tabela Original de Inferência (Assumindo que contém todas as colunas originais)\n",
    "tabela_original = spark.table(\"workspace.churn_zero.inference_gold\") \n",
    "\n",
    "# 2. Realizar o JOIN (Inner Join)\n",
    "# O join é baseado no customerID, adicionando a coluna 'churn_probability' à tabela original\n",
    "df_com_probabilidade = tabela_original.join(\n",
    "    results_df_spark,\n",
    "    on=\"customerID\",\n",
    "    how=\"inner\" # Garante que apenas clientes presentes em ambos os lados sejam mantidos\n",
    ")\n",
    "\n",
    "# 3. Visualizar o resultado final\n",
    "print(\"\\nPrévia da Tabela Original com a Coluna de Probabilidade Adicionada:\")\n",
    "df_com_probabilidade.select(\"customerID\", \"Partner\", \"Churn_Probability\").show(5)\n",
    "\n",
    "# 4. Salvar a Tabela Final\n",
    "# Substitua pelos nomes de destino\n",
    "CATALOGO_FINAL = \"workspace\"\n",
    "SCHEMA_FINAL = \"churn_zero\"\n",
    "TABELA_INFERENCIA_FINAL = \"inference_churn\"\n",
    "\n",
    "df_com_probabilidade.write.mode(\"overwrite\").saveAsTable(f\"{CATALOGO_FINAL}.{SCHEMA_FINAL}.{TABELA_INFERENCIA_FINAL}\")\n",
    "print(f\"\\nTabela final salva em: {TABELA_INFERENCIA_FINAL}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_predict",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
