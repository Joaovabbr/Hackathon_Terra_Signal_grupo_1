{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b1e1b8-27a1-4738-aace-17894028f8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Substitua pelos nomes reais do seu catálogo, schema e tabela\n",
    "CATALOGO = \"workspace\"\n",
    "SCHEMA = \"churn_zero\"\n",
    "TABELA = \"inference_silver\"\n",
    "TABELA_GOLD = \"inference_gold\"\n",
    "\n",
    "# Carregar a tabela do Unity Catalog\n",
    "df = spark.table(f\"{CATALOGO}.{SCHEMA}.{TABELA}\")\n",
    "\n",
    "print(f\"DataFrame carregado com {df.count()} linhas.\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "200bb57a-6175-4ad1-ac17-193f360f8168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_quali = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'sentiment_label','feedback_topic','sentiment_score']\n",
    "colunas_quant = ['tenure', 'MonthlyCharges', 'TotalCharges', 'MonthlyIncome']\n",
    "distinct_values = {}\n",
    "for coluna in colunas_quali:\n",
    "    distinct_values[coluna] = [row[0] for row in df.select(coluna).distinct().collect()]\n",
    "display(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd3be3a-244d-491d-af22-5008a97e7716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower\n",
    "\n",
    "colunas_binarias = [\n",
    "    'Partner', 'Dependents', 'OnlineSecurity', 'OnlineBackup', \n",
    "    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "    'PaperlessBilling', 'MultipleLines', \"PhoneService\"\n",
    "]\n",
    "df_binario = df\n",
    "for c in colunas_binarias:\n",
    "    df_binario = df_binario.withColumn(\n",
    "        c,\n",
    "        when(lower(col(c)) == 'yes', '1')\n",
    "        .when(lower(col(c)) == 'no', '0')\n",
    "    )\n",
    "df_binario = df_binario.withColumn('gender', when(col('gender') == 'Male', 1).when(col('gender') == 'Female', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254cffd9-48e1-44bd-a779-ed1ff006ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765138405967}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_binnig = df_binario\n",
    "df_binnig = df_binnig.withColumn(\n",
    "    \"tenure\",\n",
    "    when(col(\"tenure\") <= 5, \"1_muito_novo_muitoAltoRisco\") \n",
    "    .when((col(\"tenure\") > 5) & (col(\"tenure\") <= 17), \"2_novo_riscoAlto\") \n",
    "    .when((col(\"tenure\") > 17) & (col(\"tenure\") <= 43), \"3_estável_riscoMedio\")  \n",
    "    .otherwise(\"4_Leal_BaixoRisco\") \n",
    ")\n",
    "display(df_binnig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93db28f-7128-4ed0-9b83-04977ff887be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "COLUNA_ALVO = \"MonthlyIncome\"\n",
    "NUM_FAIXAS = 4\n",
    "NOVA_COLUNA = \"monthlyIncome_segment\"\n",
    "\n",
    "windowSpec = Window.orderBy(col(COLUNA_ALVO))\n",
    "df_binnig = df_binnig.withColumn(\n",
    "    NOVA_COLUNA,\n",
    "    ntile(NUM_FAIXAS).over(windowSpec)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_binnig.groupBy(NOVA_COLUNA).count().orderBy(NOVA_COLUNA)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3872208f-eeaf-495a-a9ca-3cb696a44fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "COLUNA_ALVO = \"MonthlyCharges\"\n",
    "NUM_FAIXAS = 4\n",
    "NOVA_COLUNA = \"monthlyCharges_segment\"\n",
    "\n",
    "windowSpec = Window.orderBy(col(COLUNA_ALVO))\n",
    "df_binnig = df_binnig.withColumn(\n",
    "    NOVA_COLUNA,\n",
    "    ntile(NUM_FAIXAS).over(windowSpec)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_binnig.groupBy(NOVA_COLUNA).count().orderBy(NOVA_COLUNA)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f23ee65-46ae-471d-a05d-2df3b7aba970",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765138617291}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "COLUNA_ALVO = \"TotalCharges\"\n",
    "NUM_FAIXAS = 4\n",
    "NOVA_COLUNA = \"totalCharges_segment\"\n",
    "\n",
    "windowSpec = Window.orderBy(col(COLUNA_ALVO))\n",
    "df_binnig = df_binnig.withColumn(\n",
    "    NOVA_COLUNA,\n",
    "    ntile(NUM_FAIXAS).over(windowSpec)\n",
    ")\n",
    "\n",
    "display(\n",
    "    df_binnig.groupBy(NOVA_COLUNA).count().orderBy(NOVA_COLUNA)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c58fd6b-e5d9-4c17-9248-8d847309c3e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Converter o DataFrame Spark para Pandas\n",
    "# ATENÇÃO: Se o DataFrame for muito grande, esta linha falhará.\n",
    "pandas_df = df_binnig.toPandas()\n",
    "\n",
    "# 2. Aplicar o get_dummies do Pandas\n",
    "colunas_ohe = [\n",
    "    \"InternetService\", \n",
    "    \"Contract\", \n",
    "    \"PaymentMethod\",\n",
    "    \"feedback_topic\",\n",
    "    \"tenure\",\n",
    "    \"monthlyIncome_segment\",\n",
    "    \"monthlyCharges_segment\",\n",
    "    \"totalCharges_segment\"\n",
    "]\n",
    "\n",
    "pandas_df_ohe = pd.get_dummies(pandas_df, columns=colunas_ohe)\n",
    "\n",
    "# 3. (Opcional) Converter de volta para Spark (se precisar continuar no ambiente Spark)\n",
    "df_OHE = spark.createDataFrame(pandas_df_ohe)\n",
    "\n",
    "print(\"One-Hot Encoding via Pandas concluído.\")\n",
    "df_OHE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd73ea41-2f72-49bc-a9da-2709ad6cac40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "int_columns = [\"gender\",\"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \n",
    "               \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \n",
    "               \"StreamingTV\", \"StreamingMovies\", \"PaperlessBilling\", \"sentiment_score\"]\n",
    "\n",
    "# 1. Inicialize a variável final com o DataFrame original\n",
    "df_processed = df_OHE \n",
    "\n",
    "# 2. Itere e REATRIBUA o resultado a df_processed em cada passo\n",
    "for c in int_columns:\n",
    "  if c in df_processed.columns: # Verifique a existência na variável que está sendo transformada\n",
    "    # Aplica o cast e SALVA DE VOLTA no df_processed para o próximo passo\n",
    "    df_processed = df_processed.withColumn(c, col(c).cast(\"int\")) \n",
    "\n",
    "# 3. Use o resultado final\n",
    "df_final = df_processed # Opcional: renomear para df_final\n",
    "\n",
    "print(\"Schema após o cast corrigido:\")\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b78981ed-d794-4ae1-a5c9-1c684ff9e614",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765120786611}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colunas_para_dropar = [ \n",
    "    \"TotalCharges\",\n",
    "    \"MonthlyIncome\",\n",
    "    \"CustomerFeedback\",\n",
    "    \"sentiment_label\"\n",
    "]\n",
    "\n",
    "\n",
    "df_final = df_final.drop(*colunas_para_dropar)\n",
    "\n",
    "print(\"Colunas removidas com sucesso!\")\n",
    "print(\"\\nNovo Schema do DataFrame (apenas as primeiras colunas):\")\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3e81ae-6f76-4b5d-9e2b-8c81f3cf79d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "\n",
    "# Lista de caracteres inválidos que você deve remover/substituir\n",
    "# ( ) , ; { } \\n \\t = e espaços\n",
    "CARACTERES_INVALIDOS = r'[ ,;{}\\n\\t=()]'\n",
    "CARACTERE_SUBSTITUTO = '_'\n",
    "\n",
    "# 1. Obter todos os nomes de colunas atuais\n",
    "colunas_atuais = df_final.columns\n",
    "\n",
    "# 2. Iterar sobre as colunas e renomear\n",
    "for nome_antigo in colunas_atuais:\n",
    "    # 2.1. Substituir caracteres inválidos por underscore\n",
    "    nome_novo = re.sub(CARACTERES_INVALIDOS, CARACTERE_SUBSTITUTO, nome_antigo)\n",
    "    \n",
    "    # 2.2. Remover underscores duplicados ou no início/fim\n",
    "    nome_novo = nome_novo.strip(CARACTERE_SUBSTITUTO) # Remove _ no início/fim\n",
    "    nome_novo = re.sub(r'_{2,}', CARACTERE_SUBSTITUTO, nome_novo) # Remove múltiplos __\n",
    "\n",
    "    # 3. Renomear a coluna no DataFrame (se o nome foi alterado)\n",
    "    if nome_antigo != nome_novo:\n",
    "        # Renomeia a coluna no DataFrame\n",
    "        df_final = df_final.withColumnRenamed(nome_antigo, nome_novo)\n",
    "        \n",
    "        # Opcional: imprimir a alteração para monitoramento\n",
    "        print(f\"Renomeado: '{nome_antigo}' -> '{nome_novo}'\")\n",
    "\n",
    "print(\"\\nNomes das colunas limpos e prontos para o Unity Catalog.\")\n",
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7620433-d9d8-4621-8616-f321883d0b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOGO}.{SCHEMA}.{TABELA_GOLD}\")\n",
    "\n",
    "print(f\"\\nTabela '{TABELA_GOLD}' salva no Unity Catalog e pronta para o treinamento do modelo!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8178959444653028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "inference_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
